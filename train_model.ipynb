{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zb4C1V2Adv4"
      },
      "source": [
        "## Training Transformer-based model on Arabic poetry\n",
        "By Mohammed Alneyadi (December 2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVOqNGwGA_c9"
      },
      "source": [
        "### 0. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4zvLQDTdHxS",
        "outputId": "ddf368ed-8395-465d-aeb4-c6f30157c69e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7aca9c25e130>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Z0xnZLdB8X"
      },
      "source": [
        "### 1. Load & prepare our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2N3unl1fE8b",
        "outputId": "f0fa2a7e-0c3c-4e30-89cf-790276ce2234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters: 34,389,599\n",
            "length of unique characters: 38\n"
          ]
        }
      ],
      "source": [
        "input_file_path = 'data/preprocessed_data.txt'\n",
        "with open(input_file_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"length of dataset in characters: {len(text):,}\")\n",
        "print(f\"length of unique characters: {len(set(text))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeHhgtxHFw3Z",
        "outputId": "33dfe018-eaba-4b4b-b616-21e890f98549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   ء آ أ ؤ إ ئ ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ك ل م ن ه و ى ي\n"
          ]
        }
      ],
      "source": [
        "# unique characters that occur in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(' '.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rHxigzf1FnFf"
      },
      "outputs": [],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T3VhA1MrGEfw"
      },
      "outputs": [],
      "source": [
        "# create train and test splits (90-10 split)\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdkgqtf9HJtd"
      },
      "source": [
        "### 2. Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "03kxvsZFHJte"
      },
      "outputs": [],
      "source": [
        "parameters_sets = {\n",
        "  'set1': {\n",
        "      'n_layer':4,\n",
        "      'n_head': 4,\n",
        "      'batch_size':32,\n",
        "  },\n",
        "  'set2': {\n",
        "      'n_layer':6,\n",
        "      'n_head': 6,\n",
        "      'batch_size':64,\n",
        "  },\n",
        "  'set3': {\n",
        "      'n_layer':8,\n",
        "      'n_head': 8,\n",
        "      'batch_size':128,\n",
        "  }}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RDs46dKIHJte"
      },
      "outputs": [],
      "source": [
        "# fixed hyperparameters\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOWxVcfzGZwe"
      },
      "source": [
        "### 3. Design our transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MVwNy9jYGXH4"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, batch_size):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "elfBkTvaHXQU"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(batch_size):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_Db2jfjaHgY6"
      },
      "outputs": [],
      "source": [
        "# self-attention layer\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aw1O53DcHzd7"
      },
      "outputs": [],
      "source": [
        "# multi-head attention layer\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OVuKAL2TH7r2"
      },
      "outputs": [],
      "source": [
        "# feed-forward layer\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ecqiIKNGIDFC"
      },
      "outputs": [],
      "source": [
        "# transformer block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H87ILIJ8IL1D"
      },
      "outputs": [],
      "source": [
        "# simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, n_head, n_layer):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNhduzx-XhF"
      },
      "source": [
        "### 4. Train our models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYoQlud1Gg0v"
      },
      "source": [
        "#### A. Using parameter set 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YmObby3JcSD",
        "outputId": "d0b1eef5-c924-4351-bba2-a997b736b948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.221542 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel(n_head = parameters_sets['set1']['n_head'],\n",
        "                            n_layer = parameters_sets['set1']['n_layer'])\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzxhlDWJjy-",
        "outputId": "96681f44-a1d7-4540-beb3-6b252c8e9741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 3.8395, val loss 3.8380\n",
            "step 250: train loss 2.6483, val loss 2.6426\n",
            "step 500: train loss 2.3094, val loss 2.3037\n",
            "step 750: train loss 2.1512, val loss 2.1454\n",
            "step 1000: train loss 2.0356, val loss 2.0270\n",
            "step 1250: train loss 1.9739, val loss 1.9683\n",
            "step 1500: train loss 1.9286, val loss 1.9265\n",
            "step 1750: train loss 1.8939, val loss 1.8895\n",
            "step 2000: train loss 1.8723, val loss 1.8675\n",
            "step 2250: train loss 1.8487, val loss 1.8399\n",
            "step 2500: train loss 1.8346, val loss 1.8303\n",
            "step 2750: train loss 1.8250, val loss 1.8181\n",
            "step 3000: train loss 1.8071, val loss 1.8013\n",
            "step 3250: train loss 1.8005, val loss 1.7957\n",
            "step 3500: train loss 1.7881, val loss 1.7832\n",
            "step 3750: train loss 1.7811, val loss 1.7809\n",
            "step 4000: train loss 1.7708, val loss 1.7702\n",
            "step 4250: train loss 1.7679, val loss 1.7690\n",
            "step 4500: train loss 1.7579, val loss 1.7551\n",
            "step 4750: train loss 1.7534, val loss 1.7514\n",
            "step 4999: train loss 1.7452, val loss 1.7463\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(batch_size = parameters_sets['set1']['batch_size'])\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train', batch_size = parameters_sets['set1']['batch_size'])\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpog-8FYJN7a",
        "outputId": "ae5261ff-7159-4d68-dac1-612b89b456d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "فرح نفوس أدريسهم بسيدهم    زينت كما قوم قد طلبت علمي\n",
            "لم أفني بالدهر علم فرطه    أنرخ علم خلقا لا تنمكا ظلمي\n",
            "من سباب بار شمس الكمال جازهرهم    إذا هو والتجزى من حلمهم\n",
            "فذى امر منها في فرط شماله    تولى اليك يناها لشرع عوانهم\n",
            "بمعانق تخفى بها وكادت من الصبا    وكل بغدتي عن سما أسمى عجمتي\n",
            "وكلا عمرتي لقلبي فلذلتي    شخفك تحت لا عيشي تكادمتي\n",
            "وقلبي لغيابي اخفروا مني أهلي يا    نيدم بديعي داعي بالتحرمن النصي\n",
            "فسرى وهيهة تضربا من    وجوه ناعي عن تمكيني بمينهم\n",
            "قد بعدت من وصابات الله لنوى    \n",
            "قمرات تلك الوصال بعد بعضنهم    كما لا تردو بكمال الجميل جواري\n",
            "عمان على مقالي دعات أقوى أساطعي    يسيد به الصواب هذي فأيامي قالم\n",
            "\n",
            "أليس قرنا يافنا قل سما عيدي أضرما    إلى لك فللصب في ظه\n",
            "واحمل أمره ما هو زينب جوبي صباح روعة    \n",
            "لا ترق يضيء البقاء لذ تضرئب ندو العلى    وليس نهاضا أو تمر نختدر براح لفرس\n",
            "أقراني بار ألتقيه كم فخز عزه    والله قول توله والفن يفطنه القطع موزج\n",
            "أنى والأرض بالبيضاء والمفخر لنصرا    أكرم بقرتني بكأ من قر وظلز\n",
            "أيهمن بحاه وما بين كل كمالي    فيلحق به وأمحى تحصيح نقالي\n",
            "أمن قبلكم قد طاعت من فارق    ترى يتجاه على شيء هوله السر عين ريه الغرط\n",
            "رماه وباسم فيكسر مبتسما    حيث الرمد للفضائل رمان امريض\n",
            "نالقت قدمه أيام أنما    مالي بمع فقد تلوي الرماح إحصا\n",
            "ولا تلوي رياتي على    \n",
            "غر أغال من شعب جهل المين بجسير    والكاشمين هب العزيز والذعر\n",
            "وسوى طه سير الوادي تعساب سلامه    وترجو مصاحبا في فاضحة وإن هبروض\n",
            "واندفع في عد الوحش قلبي النذير    هامات أرباب الخوفق في القذا\n",
            "وقد بد عدما فصل منه بلاقغة    وقاطع بالماء عدا فليس غلبت ودجري\n",
            "حزبته فانثني وغرام هموركه    والقتل حازهما لنا وجهق صبوره\n",
            "وكانا الحتف في الهناء جقرة    فكأن هوى الغنى انهوة وحسير\n",
            "وكأنكم كالجميل اهدي صبور الهجر ولا عن الخلق    إذ تلوم عنه اعطيت مستحور\n",
            "ولكم أضعفت اجىحأتهم سحاء    لك ذاك الشمس للوجد خلاء صبور\n",
            "بين ولا تقر جبينكم إنك مضطرب    ومن بين مر فيكم بهو الكمرط\n",
            "تحية العز المطل قد علا واسق    وعرفت هلا وا كتار له القخر من حسره    بمن عفو له قتال ولكن يخيب\n",
            "حازنت عن وآخري إلا كلام به    لنهواكم سماحا أو قريب\n",
            "ذا كليم عالم جيد وأمضي    جمالي عليه عنجز فكره نفسه\n",
            "هل هوى بالذو لحكم زماني    حاسبا فجانحا عقيلا تفلق\n",
            "أسباقك ما دع \n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyLfwsfGwDF"
      },
      "source": [
        "#### B. Using parameter set 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U3Vx_MrTlqS",
        "outputId": "7709e3a1-05f1-40e9-a6a9-f3663a53e59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.768166 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel(n_head = parameters_sets['set2']['n_head'],\n",
        "                            n_layer = parameters_sets['set2']['n_layer'])\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jOXgo4mTlqg",
        "outputId": "f2dda74a-4ec8-43d6-bdbc-0e843830b5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 3.8332, val loss 3.8349\n",
            "step 250: train loss 2.6547, val loss 2.6496\n",
            "step 500: train loss 2.2920, val loss 2.2885\n",
            "step 750: train loss 2.0793, val loss 2.0743\n",
            "step 1000: train loss 1.9658, val loss 1.9607\n",
            "step 1250: train loss 1.8920, val loss 1.8883\n",
            "step 1500: train loss 1.8421, val loss 1.8388\n",
            "step 1750: train loss 1.8118, val loss 1.8088\n",
            "step 2000: train loss 1.7855, val loss 1.7829\n",
            "step 2250: train loss 1.7662, val loss 1.7653\n",
            "step 2500: train loss 1.7518, val loss 1.7476\n",
            "step 2750: train loss 1.7338, val loss 1.7344\n",
            "step 3000: train loss 1.7224, val loss 1.7234\n",
            "step 3250: train loss 1.7119, val loss 1.7158\n",
            "step 3500: train loss 1.6985, val loss 1.7011\n",
            "step 3750: train loss 1.6898, val loss 1.6924\n",
            "step 4000: train loss 1.6817, val loss 1.6853\n",
            "step 4250: train loss 1.6727, val loss 1.6739\n",
            "step 4500: train loss 1.6619, val loss 1.6647\n",
            "step 4750: train loss 1.6513, val loss 1.6555\n",
            "step 4999: train loss 1.6447, val loss 1.6501\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(batch_size = parameters_sets['set2']['batch_size'])\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train', batch_size = parameters_sets['set2']['batch_size'])\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PdGTeCSTlqg",
        "outputId": "f63dc3b1-7403-4ef2-d488-45d3a969f911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "كلام اسم لشكوى ووبلة العلا    تتقته نوارا وبأد قبا\n",
            "وأشكال كأسكم في خضاب    وأطوع قلب منكم سبا\n",
            "تقول الأمم بالخكلة والتو    وباللحطة ختم وذهبا\n",
            "ولدها فضحى غوايا ختمان    من والدنيا بجميع أجمعها\n",
            "وقد كنت بنادية بين النبي    ووليت بالاقتضان قبل شهبا\n",
            "قرأت محتدا من فرش الشهب    وحلما حاسرا في العزب عضبا\n",
            "وان قبل من جنود سليم    وارتع الى والكنز قد خرقا\n",
            "وابتبعت الرى قبلي فيهم    روحي فكل الجليت بأبلحا\n",
            "لو انجم بين زهر فيهم    من يومه فراش الجمع مرهبا\n",
            "إذ تقيهم بالنسج الذكريا    ما اغتنم يحمل باتفا\n",
            "كل يوم امحص عنه فغير لي    باحات ياما وبان الحروبا\n",
            "وشدل لأنملق وجهه    صوبا وهو ذو الإنسان تضرك\n",
            "وقد نزعنا ذي يومه    وو ارتياح لقلبي منك النوبا\n",
            "حطام النفس فكرول لتجاريا    وليس لولا الغوار أمول مغرب\n",
            "ذو الرأى ما قله مأنوب شعب    وعند لم يبدع ملوكا مولمونه\n",
            "ابنفوا بالمكارم من ليس مكارم    والكريم الجسم مؤروما وحسبك مركب\n",
            "إن أهلي لائم ورداء فلم تتك    لبدر في الأفلاك الأنابة والصبا\n",
            "كودي ذو البلبس الكريم اليوم حلوه    يقبل الستر بلباب اللبس متازب\n",
            "سبحانها بكر اليقين قليل    فلا يغفل فودفعها مجرون قلبي\n",
            "دوحي بنا جنح المجد الحكيم    يمضي رشا معقلي إليه صدي\n",
            "قامت من الحلماء مري وكله    نزوحا يبدو الذي عقبا\n",
            "سلم البلاد وفي زاده    دوحي جلوس عجاجته مضرونتي\n",
            "فعذاره بكراته الهتوم    عذري وعلى الدهر الأنوح الكبي\n",
            "هو الفادة البخل وحدها إل    يكون إذا ما هبطوا القمري\n",
            "ومولاي بنفثات آكل حفظ    وحديثا بقيث ومهج كوني\n",
            "ملء الكمام وسيس الزهر ود    أقراطه البيان كشت بأسيس\n",
            "أفؤاده كدر بدلة الصفاب    ملء الحرير نعي أطيس المعاني\n",
            "ولطل أثر قلب قال وإن    ثارت في أم صدر ولم أر\n",
            "ملء فؤاد بعد القلب منجم    رؤى صناع الله نزل الأحل\n",
            "يا ملئ ناظما في الكون تعجير    في صناعة النذل بعد الدهنهء\n",
            "فاذا تولى صدرك أيام واذخ    وسط راحلا في غمرة الإقداء\n",
            "تحملني شكل واثقال النوى    ولحظك من صان ولم جاورا\n",
            "صانع يقين معربة في السماو    يموق عين البين هدنا وانطوى\n",
            "كم في الذي يطوي بغمر ضميري    إلا وشقيقه في اقتحار هوى\n",
            "للزاب بوالده كم من نداه    على حاسباته الأدبار قد عبات\n",
            "ما بذلك الرحمن إن قال نداه    ولم يجب عن فرق يخال العنقا\n",
            "أهلا تزين الحاطين طالما    مجدا تزاير قرى ليس ذو ذرقات\n",
            "فارحم أن يملي بهم خوفه    رين الزعاما لاقاء سوع الجاذبات\n",
            "لا سيم\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwLkRID0TsjZ"
      },
      "source": [
        "#### C. Using parameter set 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNk7284vTsjj",
        "outputId": "22c8e8a4-04ce-4de2-b960-34edd2c57b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14.31479 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel(n_head = parameters_sets['set3']['n_head'],\n",
        "                            n_layer = parameters_sets['set3']['n_layer'])\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgeoi9ZKTsjj",
        "outputId": "980ee519-cdbe-4776-b4a2-908df95f7771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 3.7908, val loss 3.7897\n",
            "step 250: train loss 2.6599, val loss 2.6541\n",
            "step 500: train loss 2.2342, val loss 2.2295\n",
            "step 750: train loss 1.9939, val loss 1.9875\n",
            "step 1000: train loss 1.8869, val loss 1.8857\n",
            "step 1250: train loss 1.8228, val loss 1.8183\n",
            "step 1500: train loss 1.7812, val loss 1.7815\n",
            "step 1750: train loss 1.7520, val loss 1.7515\n",
            "step 2000: train loss 1.7343, val loss 1.7353\n",
            "step 2250: train loss 1.7081, val loss 1.7097\n",
            "step 2500: train loss 1.6925, val loss 1.6958\n",
            "step 2750: train loss 1.6750, val loss 1.6788\n",
            "step 3000: train loss 1.6586, val loss 1.6615\n",
            "step 3250: train loss 1.6453, val loss 1.6502\n",
            "step 3500: train loss 1.6304, val loss 1.6379\n",
            "step 3750: train loss 1.6150, val loss 1.6214\n",
            "step 4000: train loss 1.5984, val loss 1.6085\n",
            "step 4250: train loss 1.5850, val loss 1.5957\n",
            "step 4500: train loss 1.5710, val loss 1.5834\n",
            "step 4750: train loss 1.5586, val loss 1.5702\n",
            "step 4999: train loss 1.5477, val loss 1.5616\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(batch_size = parameters_sets['set3']['batch_size'])\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train', batch_size = parameters_sets['set3']['batch_size'])\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4geh8ArTsjk",
        "outputId": "33d96071-055d-42b3-a1a4-962e10241bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "تعلم أن تشكو لشأوك قولهم حتى    تجب إلى النار اني بالمجامين والنين\n",
            "إلى أنه ما قال لي غصن عتيقة    وعن غايتي يشكون في الخل والتين\n",
            "يطل على الهاتي والوغى حادثا وهوى    يلوح بيثوب الذكر بالدموع تمامين\n",
            "ولي بعد محمود لك الصيد جازعا    خطابين بين العالمين الدين\n",
            "منار يريناه بالجو مجنبه    والمجتري ببدا هو العيون\n",
            "ودام على أيى ابن الحمام ففي    كل الضمير أبا الإعدام تميين\n",
            "وأنت أدري ثورة تبدي لذرة    لولا ارفعت ثم تكبين نكيين\n",
            "أم قد سلبت عن كبير الخلق فانثني    وكنت أكبر للرشاد توكيني\n",
            "وقد بدا لمخلوق وسبر تدنى علا    سبق الحسين إذا ما بدا الحدثين\n",
            "ومن عجيب دنا خيلا على أين    وأصول قبل أودى بالمتين\n",
            "ومن حقيقة لم يهمره الله لدى    مصر بل من ضرام الجن والبهن\n",
            "ومن بأبيك آل الكذبيات سامي    والجوم يحتمل بالحزن الرنين\n",
            "فلقد جزات والسهو يمين على    قدس الأساوين بالدع والنحن\n",
            "ومن يكون الموت بالخمرة التي    لم يحسن الآيات يشهى للسابغة\n",
            "واحذر بنصر الأرض بالحر البلا    أني أقول الأرض فبالإنسان\n",
            "ومها للفضل أهلى بالتنبي وقا    وسؤال بالتمويه وأضعاني\n",
            "ورب المبادي والفضل الذي به    شاء الغناءين فيه أيمان\n",
            "قد يختالبون بالندى شاءوا لنا    منعم الرحمن على النعمان\n",
            "ما يدركوني إلا يد المنجد    حتما من مقارن الرحمد\n",
            "فصحوة الفقار قد ذكروها    دون أنفسها وروع الأوكان\n",
            "وعسى ألف لشامخ قبرا    يشتهي عن مهجتها ألوان\n",
            "غضبتها باعك حتى كنا    لم ندر بعد أيدي طولاني\n",
            "شهدة وسعدة وشأن    لا عدالة ودعاني\n",
            "عاهدي فقد ألمعيهم    أشجعين سر دمع الثاني\n",
            "فإذا تحكم ضروب    ولهم للتصديدا\n",
            "لو ماجد قمر الغمام أشد    ملقى قيانته في ساني\n",
            "يا قبر إن لم إن حرم السما    ل سفن قدسى لهفو أجن\n",
            "أم ريضك أعجب من علاه    قل  إن شاء غدته راني\n",
            "أم انتظمت حيث الدراري    قوما بجدك لاستوابي\n",
            "وبحسنهمي قد ينال الظنون    وأوابذ أخرته أرعاس فراق\n",
            "من رأى في حل ملاهي روما    ب ذرى الأريج واستواب الرواق\n",
            "وسهم سواك ما ترعوى هي    ل على القلب مغني الغيث\n",
            "ولما رزأت جفونا العالمين    حاشواك حين عاش الجاه كئيب\n",
            "وكنت بأهيم اليأس الرفيق    تأسد الثرى لسلسلي الرفيب\n",
            "رفناه كيف أرى نفحات قلبي    فجنان الصبابة كل الصبيب\n",
            "رعدت م نكبة على ذاته    وهي النويلك تظمئني بالخطوب\n",
            "ليس شيء يرى من كل عذر    كلما ألحق بباليس مني\n",
            "وهي وقد يراه ظاعن قد فدا    ليس يراها المنى روح وطيب\n",
            "أيها الفذ ملجا\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mVOqNGwGA_c9",
        "T0Z0xnZLdB8X",
        "kdkgqtf9HJtd",
        "UOWxVcfzGZwe",
        "qPNhduzx-XhF",
        "LYoQlud1Gg0v",
        "dEyLfwsfGwDF",
        "VwLkRID0TsjZ"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
